"""
Adapter para comunica√ß√£o com APIs de IA
Seguindo princ√≠pios de Clean Architecture
"""
import time
import random
from typing import Dict, List, Optional
from abc import ABC, abstractmethod
from pathlib import Path
from openai import OpenAI
import streamlit as st

# Importa√ß√µes para embeddings (opcionais)
try:
    from langchain_community.vectorstores import FAISS
    from langchain_openai import OpenAIEmbeddings
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    LANGCHAIN_AVAILABLE = True
    print("Langchain est√° instalado")
except ImportError:
    print("Langchain n√£o est√° instalado")
    LANGCHAIN_AVAILABLE = False


class AIProviderInterface(ABC):
    """Interface abstrata para provedores de IA"""
    
    @abstractmethod
    def generate_response(self, message: str, model: str) -> str:
        """Gera uma resposta baseada na mensagem do usu√°rio"""
        pass
    
    @abstractmethod
    def get_available_models(self) -> List[str]:
        """Retorna lista de modelos dispon√≠veis"""
        pass


class OpenAIAdapter(AIProviderInterface):
    """Adapter para OpenAI API com suporte a embeddings"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key
        self.available_models = [
            "o1"
        ]
        self.vector_store = None
        self.embeddings_cache_dir = Path("embeddings_cache")
        self.embeddings_cache_dir.mkdir(exist_ok=True)
        self.documents_dir = Path("documents")
        
        # Inicializa embeddings se LangChain estiver dispon√≠vel
        if LANGCHAIN_AVAILABLE and api_key:
            self.embeddings = OpenAIEmbeddings(api_key=api_key)
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            self._load_or_create_embeddings()
    
    def _load_or_create_embeddings(self):
        """Cache inteligente: carrega embeddings existentes ou cria novos"""
        cache_path = self.embeddings_cache_dir / "tcross_embeddings"
        
        # Tenta carregar embeddings existentes
        if cache_path.exists():
            try:
                self.vector_store = FAISS.load_local(
                    str(cache_path), 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print("‚úÖ Embeddings carregados do cache")
                return
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao carregar embeddings do cache: {e}")
        
        # Se n√£o conseguir carregar, cria novos embeddings
        self._create_embeddings_from_txt_files()
    
    def _create_embeddings_from_txt_files(self):
        """Cria embeddings a partir de arquivos .txt na pasta documents"""
        if not self.documents_dir.exists():
            self.documents_dir.mkdir()
            print("üìÅ Pasta 'documents' criada. "
                  "Adicione arquivos .txt l√° para usar embeddings.")
            return
        
        txt_files = list(self.documents_dir.glob("*.txt"))
        if not txt_files:
            print("üìÑ Nenhum arquivo .txt encontrado na pasta 'documents'")
            return
        
        # Carrega todos os documentos
        all_texts = []
        for file_path in txt_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # Adiciona metadados
                    filename = file_path.name
                    content_with_metadata = (f"[ARQUIVO: {filename}]\n"
                                           f"{content}")
                    
                    # Divide em chunks
                    chunks = self.text_splitter.split_text(
                        content_with_metadata)
                    all_texts.extend(chunks)
                    
                print(f"üìñ Processado: {filename}")
            except Exception as e:
                print(f"‚ùå Erro ao processar {file_path}: {e}")
        
        if all_texts:
            try:
                # Cria vector store
                self.vector_store = FAISS.from_texts(
                    all_texts, self.embeddings)
                
                # Salva no cache
                cache_path = self.embeddings_cache_dir / "tcross_embeddings"
                self.vector_store.save_local(str(cache_path))
                
                print(f"‚úÖ Embeddings criados com {len(all_texts)} chunks "
                      f"e salvos no cache")
            except Exception as e:
                print(f"‚ùå Erro ao criar embeddings: {e}")
    
    def _get_context_from_embeddings(self, message: str, k: int = 3) -> str:
        """Busca contexto relevante nos embeddings"""
        if not self.vector_store:
            return ""
        
        try:
            docs = self.vector_store.similarity_search(message, k=k)
            if docs:
                context_parts = [doc.page_content for doc in docs]
                return ("\n\nCONTEXTO DOS DOCUMENTOS:\n" + 
                        "\n---\n".join(context_parts))
        except Exception as e:
            print(f"‚ö†Ô∏è Erro na busca por similaridade: {e}")
        
        return ""
    
    def generate_response(self, message: str, model: str) -> str:
        """
        Gera resposta usando OpenAI API com contexto de embeddings
        """
        client = OpenAI(api_key=self.api_key)
        
        # Busca contexto relevante nos embeddings
        context = (self._get_context_from_embeddings(message) 
                   if LANGCHAIN_AVAILABLE else "")
        
        # Prepara mensagens do sistema
        messages = [
            {"role": "system", 
             "content": "Voc√™ √© um engenheiro da Volkswagen que responde "
                        "perguntas sobre o VW T-Cross."},
            {"role": "system", 
             "content": f"O carro atualmente selecionado √© um VW T-Cross "
                        f"{st.session_state.ano_veiculo} "
                        f"{st.session_state.versao_veiculo}"},
            {"role": "system", 
             "content": "Voc√™ deve responder apenas perguntas sobre o "
                        "VW T-Cross, caso seja sobre um outro carro ou um "
                        "outro contexto, diga que voc√™ n√£o pode responder."}
        ]
        
        # Adiciona contexto dos documentos se dispon√≠vel
        if context:
            messages.append({
                "role": "system", 
                "content": f"Use as informa√ß√µes dos documentos abaixo "
                           f"como refer√™ncia para suas respostas:{context}"
                           f"N√£o responda nenhuma pergunta cujo a resposta n√£o esteja no contexto dos documentos."
            })
        
        messages.append({"role": "user", "content": message})
        
        response = client.chat.completions.create(
            model=model,
            messages=messages
        )
        return response.choices[0].message.content
        
        return self._simulate_openai_response(message, model)
    
    def get_available_models(self) -> List[str]:
        """Retorna modelos dispon√≠veis da OpenAI"""
        return self.available_models
    
    def _simulate_openai_response(self, message: str, model: str) -> str:
        """Simula resposta da OpenAI para demonstra√ß√£o"""
        
        # Respostas espec√≠ficas para o contexto do VW T-Cross
        tcross_responses = {
            "consumo": (
                "O VW T-Cross tem consumo m√©dio de 14,5 km/l na cidade "
                "e 16,2 km/l na estrada com motor 1.0 TSI. "
                "Com motor 1.4 TSI, o consumo √© de 13,8 km/l cidade "
                "e 15,1 km/l estrada."
            ),
            "pre√ßo": (
                "O pre√ßo do VW T-Cross varia de R$ 85.000 a R$ 130.000, "
                "dependendo da vers√£o (Sense, Comfortline, Highline). "
                "Consulte uma concession√°ria para valores atualizados."
            ),
            "ficha t√©cnica": (
                "VW T-Cross principais especifica√ß√µes:\n"
                "- Motores: 1.0 TSI (128cv) ou 1.4 TSI (150cv)\n"
                "- Transmiss√£o: Manual 6 marchas ou autom√°tica 6 marchas\n"
                "- Comprimento: 4,19m\n"
                "- Porta-malas: 420 litros"
            ),
            "vers√µes": (
                "O VW T-Cross tem 3 vers√µes principais:\n"
                "‚Ä¢ Sense: vers√£o de entrada\n"
                "‚Ä¢ Comfortline: vers√£o intermedi√°ria\n"
                "‚Ä¢ Highline: vers√£o top de linha\n"
                "Cada uma com diferentes n√≠veis de equipamentos."
            ),
            "manuten√ß√£o": (
                "A manuten√ß√£o do VW T-Cross deve ser feita a cada "
                "10.000 km ou 12 meses. O custo m√©dio das revis√µes "
                "varia entre R$ 400 a R$ 800, dependendo do tipo "
                "de servi√ßo necess√°rio."
            )
        }
        
        # Verifica se a pergunta √© sobre o T-Cross
        message_lower = message.lower()
        for key, response in tcross_responses.items():
            if key in message_lower:
                return f"[{model}] {response}"
        
        # Respostas gen√©ricas
        generic_responses = [
            (f"Como {model}, posso ajudar com informa√ß√µes sobre o "
             f"VW T-Cross. Voc√™ gostaria de saber sobre consumo, "
             f"pre√ßo, vers√µes ou manuten√ß√£o?"),
            (f"Baseado no {model}, posso fornecer detalhes t√©cnicos "
             f"e pr√°ticos sobre o VW T-Cross. O que especificamente "
             f"voc√™ gostaria de saber?"),
            (f"Usando {model}, posso esclarecer suas d√∫vidas sobre "
             f"o VW T-Cross. Pergunte sobre caracter√≠sticas, "
             f"desempenho ou qualquer aspecto do ve√≠culo.")
        ]
        
        return random.choice(generic_responses)


class ClaudeAdapter(AIProviderInterface):
    """Adapter para Claude API"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key
        self.available_models = [
            "Claude-3", "Claude-3-Sonnet", "Claude-3-Opus"
        ]
    
    def generate_response(self, message: str, model: str) -> str:
        """Gera resposta usando Claude API"""
        time.sleep(random.uniform(0.3, 1.5))
        return f"[{model}] Resposta simulada do Claude para: {message}"
    
    def get_available_models(self) -> List[str]:
        return self.available_models


class AIService:
    """Servi√ßo principal para gerenciar diferentes provedores de IA"""
    
    def __init__(self):
        self.providers: Dict[str, AIProviderInterface] = {
            "openai": OpenAIAdapter(),
            "claude": ClaudeAdapter()
        }
        self.current_provider = "openai"
    
    def set_provider(self, provider_name: str):
        """Define o provedor de IA atual"""
        if provider_name in self.providers:
            self.current_provider = provider_name
        else:
            raise ValueError(f"Provedor {provider_name} n√£o dispon√≠vel")
    
    def get_response(self, message: str, model: str) -> str:
        """Obt√©m resposta do provedor atual"""
        provider = self.providers[self.current_provider]
        return provider.generate_response(message, model)
    
    def get_available_models(self) -> List[str]:
        """Retorna modelos dispon√≠veis do provedor atual"""
        provider = self.providers[self.current_provider]
        return provider.get_available_models()
    
    def get_all_models(self) -> Dict[str, List[str]]:
        """Retorna todos os modelos de todos os provedores"""
        all_models = {}
        for name, provider in self.providers.items():
            all_models[name] = provider.get_available_models()
        return all_models


# Inst√¢ncia singleton do servi√ßo
ai_service = AIService() 